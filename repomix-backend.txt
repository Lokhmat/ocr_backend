This file is a merged representation of the entire codebase, combining all repository files into a single document.
Generated by Repomix on: 2025-05-06T10:57:16.864Z

================================================================
File Summary
================================================================

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This summary section
2. Repository information
3. Directory structure
4. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
- This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
- When processing this file, use the file path to distinguish
  between different files in the repository.
- Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and Repomix's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

Additional Info:
----------------

================================================================
Directory Structure
================================================================
app/
  src/
    auth/
      schemas.py
      security.py
    models/
      postgresql/
        image_insert.sql
        image_update_status_and_result.sql
        images_get_by_id.sql
        images_get_by_user.sql
        tokens_create.sql
        user_create.sql
        user_get_by_email.sql
      connector.py
      image.py
      token.py
      user.py
    process/
      image_processor.py
      schemas.py
    routers/
      auth_router.py
      image_router.py
      process_router.py
      token_router.py
    token/
      schemas.py
    bucket_init.py
    constants.py
    main.py
  Dockerfile
  requirements.txt
database/
  migrations/
    V01__init_schema.sql
    V02__table_users copy.sql
    V03__table_images.sql
    V04__table_tokens.sql
    V05__images_workload.sql
  Dockerfile
nginx/
  nginx.conf
qwen/
  Dockerfile
  server.py
readonly_backend/
  src/
    auth/
      schemas.py
      security.py
    models/
      postgresql/
        get_user_id.sql
        images_get_by_id.sql
        images_get_by_user.sql
      connector.py
      image.py
      schemas.py
    routers/
      read_router.py
    constants.py
    main.py
  Dockerfile
  requirements.txt
.gitignore
docker-compose.yml
LICENSE
startup.sh

================================================================
Files
================================================================

================
File: app/src/auth/schemas.py
================
from pydantic import BaseModel, EmailStr, Field

# ---------- INPUT MODELS ----------

class UserRegisterIn(BaseModel):
    email: EmailStr
    password: str = Field(min_length=6)

class UserLoginIn(BaseModel):
    email: EmailStr
    password: str = Field(min_length=6)

class RefreshTokenIn(BaseModel):
    refresh_token: str

# ---------- OUTPUT MODELS ----------

class TokenOut(BaseModel):
    access_token: str
    refresh_token: str
    token_type: str = "bearer"

class UserOut(BaseModel):
    id: int
    email: EmailStr

================
File: app/src/auth/security.py
================
import os
from datetime import datetime, timedelta, timezone
from jose import jwt, JWTError

from fastapi import Request, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials

SECRET_KEY = os.environ["JWT_SECRET_KEY"]
ALGORITHM = os.environ.get("JWT_ALGORITHM", "HS256")
ACCESS_TOKEN_EXPIRE_MINUTES = int(os.environ.get("ACCESS_TOKEN_EXPIRE_MINUTES", 15))
REFRESH_TOKEN_EXPIRE_DAYS = int(os.environ.get("REFRESH_TOKEN_EXPIRE_DAYS", 7))

security_scheme = HTTPBearer(auto_error=False)

async def get_current_user(request: Request):
    credentials: HTTPAuthorizationCredentials = await security_scheme(request)
    if credentials is None or credentials.scheme.lower() != "bearer":
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Not authenticated")

    token = credentials.credentials
    payload = verify_token(token)
    if payload is None or "sub" not in payload:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

    return payload["sub"]  # return user id

def create_access_token(data: dict):
    to_encode = data.copy()
    expire = datetime.now(timezone.utc) + timedelta(minutes=ACCESS_TOKEN_EXPIRE_MINUTES)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def create_refresh_token(data: dict):
    to_encode = data.copy()
    expire = datetime.now(timezone.utc) + timedelta(days=REFRESH_TOKEN_EXPIRE_DAYS)
    to_encode.update({"exp": expire})
    return jwt.encode(to_encode, SECRET_KEY, algorithm=ALGORITHM)

def verify_token(token: str):
    try:
        payload = jwt.decode(token, SECRET_KEY, algorithms=[ALGORITHM])
        return payload
    except JWTError:
        return None

================
File: app/src/models/postgresql/image_insert.sql
================
INSERT INTO app.images (user_id, s3_key, status, workload)
VALUES (:user_id, :s3_key, 'created', :workload)
RETURNING id, user_id, s3_key, status, workload, result_json, created_at;

================
File: app/src/models/postgresql/image_update_status_and_result.sql
================
UPDATE app.images
SET status = :status, result_json = :result_json, status_reason=:status_reason
WHERE id = :image_id;

================
File: app/src/models/postgresql/images_get_by_id.sql
================
SELECT *
FROM app.images
WHERE id = :image_id
LIMIT 1;

================
File: app/src/models/postgresql/images_get_by_user.sql
================
SELECT id, s3_key, status, result_json, created_at
FROM app.images
WHERE user_id = :user_id AND (:cursor_timestamp IS NULL OR created_at < :cursor_timestamp)
ORDER BY created_at DESC
LIMIT :limit;

================
File: app/src/models/postgresql/tokens_create.sql
================
INSERT INTO app.tokens (user_id, token, expires_at)
VALUES (:user_id, :token, :expires_at)
RETURNING token, expires_at;

================
File: app/src/models/postgresql/user_create.sql
================
INSERT INTO app.users (id, email, hashed_password)
VALUES (:user_id, :email, :hashed_password)
RETURNING id, email;

================
File: app/src/models/postgresql/user_get_by_email.sql
================
SELECT id, email, hashed_password FROM app.users WHERE email = :email;

================
File: app/src/models/connector.py
================
import os

from sqlalchemy import create_engine

DB_CONTAINER_NAME='database'

class DBConnector:
    def __init__(self):
        user = os.environ.get('PGUSER')
        password = os.environ.get('PGPASSWORD')
        host = DB_CONTAINER_NAME
        port = os.environ.get('PGPORT')
        db = os.environ.get('PGDATABASE')

        database_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'

        self.engine = create_engine(
            database_url
        )

connector = DBConnector()

================
File: app/src/models/image.py
================
from typing import List, Tuple
import json
from sqlalchemy import text
from datetime import datetime

from ..constants import BASE_POSTGRES_TRANSACTIONS_DIRECTORY
from .connector import connector
from ..process.schemas import ImageStatus

class Image:
    def create(self, user_id: str, s3_key: str, workload: str) -> dict:
        """Create a new image record in the database."""
        with connector.engine.begin() as conn:
            insert_sql = open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/image_insert.sql").read()
            query = text(insert_sql)
            result = conn.execute(query, {
                "user_id": user_id,
                "s3_key": s3_key,
                "workload": workload,
            }).fetchone()
            return {"id": result.id, "status": result.status}

    def update_status(self, image_id: str, status: str, result_json: dict = None):
        """Update the status and result of an image."""
        with connector.engine.begin() as conn:
            update_sql = open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/image_update_status_and_result.sql").read()
            query = text(update_sql)
            conn.execute(query, {
                "image_id": image_id,
                "status": status,
                "status_reason": None,
                "result_json": json.dumps(result_json) if result_json else "{}"
            })

    def update_error(self, image_id: str, error_reason: str, result_json: dict = None):
        """Update the status and result of an image."""
        with connector.engine.begin() as conn:
            update_sql = open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/image_update_status_and_result.sql").read()
            query = text(update_sql)
            conn.execute(query, {
                "image_id": image_id,
                "status": "error",
                "status_reason": error_reason,
                "result_json": json.dumps(result_json) if result_json else "{}"
            })

    def get_by_user(self, user_id: str, cursor: str = None, limit: int = 10) -> Tuple[List[ImageStatus], str]:
        """
        Get paginated images for a specific user.
        Returns: (images, next_cursor)
        """
        with connector.engine.begin() as conn:
            # If cursor is provided, parse it as timestamp
            cursor_timestamp = None
            if cursor:
                try:
                    cursor_timestamp = datetime.fromisoformat(cursor)
                except ValueError:
                    cursor_timestamp = None

            select_sql = open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/images_get_by_user.sql").read()
            query = text(select_sql)
            
            params = {"user_id": user_id, "limit": limit, "cursor_timestamp": cursor_timestamp}
            
            results = conn.execute(query, params).fetchall()
            
            # Get the next cursor (timestamp of the last record)
            next_cursor = None
            if results:
                next_cursor = results[-1].created_at.isoformat()

            images = [
                ImageStatus(
                    image_id=str(row.id),
                    s3_key=str(row.s3_key),
                    status=str(row.status),
                    result_json=str(row.result_json),
                    created_at=row.created_at
                )
                for row in results
            ]
            
            return images, next_cursor 
        
    def get_by_id(self, image_id: str) -> ImageStatus:
        """
        Get image by it's id.
        Returns: ImageStatus
        """
        with connector.engine.begin() as conn:
            select_sql = open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/images_get_by_id.sql").read()
            query = text(select_sql)
            
            params = {"image_id": image_id}
            
            result = conn.execute(query, params).fetchone()
            
            if not result:
                return None
                
            return ImageStatus(
                image_id=str(result.id),
                s3_key=str(result.s3_key), 
                status=str(result.status),
                result_json=str(result.result_json),
                created_at=result.created_at
            )

================
File: app/src/models/token.py
================
import uuid
from datetime import datetime, timedelta, timezone
from typing import Optional

from sqlalchemy import text

from ..constants import BASE_POSTGRES_TRANSACTIONS_DIRECTORY

def generate_token() -> str:
    return str(uuid.uuid4())

def create_token(connector, user_id: str, days_valid: Optional[int] = None) -> dict:
    with connector.engine.begin() as conn:
        token = generate_token()
        expires_at = None
        if days_valid is not None:
            expires_at = datetime.now(timezone.utc) + timedelta(days=days_valid)
        
        with open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/tokens_create.sql") as f:
            query = text(f.read())
            conn.execute(
                query,
                {
                    "user_id": user_id,
                    "token": token,
                    "expires_at": expires_at
                }
            ).fetchone()
        
        return {
            "token": token,
            "expires_at": expires_at.isoformat() if expires_at else None
        }

================
File: app/src/models/user.py
================
import hashlib
import uuid

from sqlalchemy import text

from ..constants import BASE_POSTGRES_TRANSACTIONS_DIRECTORY

def hash_password(password: str) -> str:
    return hashlib.sha256(password.encode("utf-8")).hexdigest()

def verify_password(plain_password: str, hashed_password: str) -> bool:
    return hash_password(plain_password) == hashed_password

def create_user(connection, email: str, password: str):
    user_id = str(uuid.uuid4())
    hashed_password = hash_password(password)
    with open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/user_create.sql") as f:
        query = text(f.read())
        return connection.execute(query, {"user_id": user_id, "email": email, "hashed_password": hashed_password}).fetchone()

def get_user_by_email(connection, email: str):
    with open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/user_get_by_email.sql") as f:
        query = text(f.read())
        return connection.execute(query, {"email": email}).fetchone()

================
File: app/src/process/image_processor.py
================
import openai
import json
import requests
import base64
import os
from typing import Dict, Any
from fastapi import HTTPException

# Configure OpenAI client for OpenRouter
client = openai.OpenAI(
    api_key=os.getenv("OPENROUTER_API_KEY"),
    base_url="https://openrouter.ai/api/v1"
)

def encode_image(image_path: str) -> str:
    """Encode image file to base64 string."""
    try:
        with open(image_path, "rb") as img_file:
            return base64.b64encode(img_file.read()).decode("utf-8")
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Failed to encode image: {str(e)}")

def extract_json_from_image_cloud(image_path: str) -> Dict[str, Any]:
    """
    Extract JSON data from an image using Qwen model.
    
    Args:
        image_path: Path to the image file
        
    Returns:
        Dictionary containing the extracted JSON data
        
    Raises:
        HTTPException: If the API call fails or returns invalid data
    """
    try:
        # Encode image
        encoded_image = encode_image(image_path)
        print("Encoded image")
        
        # Create prompt for JSON extraction
        prompt_text = """
        #Your Task: Receipt Recognition and Data Extraction

        You are tasked with extracting structured information from receipts. Receipts will come from various countries, in different languages, and can have different layouts and formats. Your goal is to parse the receipt text and return the data in JSON format with the required fields. Follow the specific instructions below to ensure accurate extraction.

        #Required Fields:

        1. Receipt Number: Extract the unique receipt number, typically found at the top of the receipt.
        2. Store/Business Name: Extract the name of the store, cafe, restaurant, or service provider.
        3. Store Address: Extract the address of the store, including city and country if available.
        4. Date: Extract the date of the transaction and format it as YYYY-MM-DD HH:MM.
        5. Currency: Extract the currency if explicitly mentioned (e.g., EUR, USD). If the currency is not specified, leave it as null.
        6. Total Amount: Extract the total amount of the transaction. This is typically located at the end of the receipt, often highlighted in bold or a larger font.
        7. Total Discount: Extract the total discount if explicitly mentioned. If not, calculate the total discount by summing up the discounts for individual items.
        8. Tax: Extract the total tax amount if it is listed on the receipt.

        #Item-Level Details:

        For each item on the receipt, extract the following details:

        1. Item Name: Extract the full name of each item. Some items may have names split across multiple lines; in this case, concatenate the lines until you encounter a quantity or unit of measurement (e.g., "2kg"), which marks the end of the item name or indicates that the item details have begun.
        2. Quantity: Extract the quantity of each item, taking into account both the numerical amount and the unit of measurement.
        3. Price: Extract the final price for each item or position on the receipt.
        4. Discount: Optionally extract any discount associated with the item, if available.

        #Handling Quantity and Units:

        When extracting quantity details, consider the following cases to correctly interpret items sold by weight versus items sold by piece:

        1. **Items Sold by Weight:**
        - **Explicit Weight Information:** If the receipt shows a numerical value with a weight unit (such as "kg", "g", or "lb"), extract that number as the weight amount and the corresponding unit as the unit of measurement.
            - *Example:* For a line "Sugar 0.5 kg", extract `"quantity": { "amount": 0.5, "unit_of_measurement": "kg" }`.
        - **Weight Unit Variations:** Recognize variations in units (e.g., "kgs", "kilograms", "grams", "lbs", "pounds"). Normalize these into one of the supported units: `kg`, `g`, or `lb`.
        - **Multi-line Formats:** If weight information is split over multiple lines or appears alongside price details (for example, "Bananas" on one line and "1.2 kg" on the next), merge the lines to correctly assign the weight amount and unit.

        2. **Items Sold by Piece:**
        - **Explicit Piece Information:** If the receipt indicates the number of pieces (often shown as "pcs" or implied by a multiplication format such as "5 * 23.00 = 115.0"), assign the unit of measurement as `"pcs"`.
            - *Example:* For a line "Eggs 12 pcs", extract `"quantity": { "amount": 12, "unit_of_measurement": "pcs" }`.
        - **Multiplication Format Cases:** When a line such as "5 * 23.00 = 115.0" appears, interpret it as 5 pieces, and look for the corresponding item name in an adjacent line. Ensure that `"quantity": { "amount": 5, "unit_of_measurement": "pcs" }` is captured.
        - **Inconsistent or Missing Quantity Information:** If the receipt implies that an item is sold by pieces (for example, through context or typical product type) but does not clearly provide a quantity, default to 1. This means if the quantity is ambiguous or absent, assume that the client bought 1 unit.

        3. **Ambiguous or Inconsistent Cases:**
        - **Missing or Unclear Units:** If a receipt does not clearly specify the unit of measurement but the context implies weight (for example, by showing decimal numbers or using terms like “kg” in nearby text), extract the weight if it is clear; otherwise, mark the unit as `"not available"` and the amount as `"unknown"`.
        - **Context-Dependent Interpretation:** In cases where the same receipt might list both weighted items and items sold by piece, use contextual clues (such as typical product types or formatting cues) to determine whether the number represents a weight or a count. If ambiguous, prioritize explicitly stated units; if none are provided and the context suggests pieces, default the quantity to 1.

        #Special Cases:

        1. Multi-line Item Names: If an item name spans multiple lines, merge the lines to form the complete name. Stop merging when a quantity or unit of measurement is encountered.
        2. Total Amount: The total amount is often larger than other numbers or displayed in bold at the bottom of the receipt. Make sure to capture this accurately.
        3. Total Discount: If no total discount is listed, sum the discounts for each individual item.
        4. Total Tax: Find the total tax amount that is paid or should be paid. It is usually found at the bottom of the receipt.
        5. Quantity Details: The quantity of an item might appear before, after, or on the same line as the item name. Use spatial and contextual cues to merge the relevant information. For example:
        - If one line shows "5 * 23.00 = 115.0" and the next line shows "Milk," interpret this as 5 pieces of Milk at a total price of 115.0.
        - Conversely, if the item name appears first (e.g., "Milk") and a following line shows "5 * 23.00 = 115.0," treat it the same way.

        #JSON Output Format:

        {
        "receipt_number": "string",
        "store_name": "string",
        "store_address": "string",
        "date_time": "string",
        "currency": "optional[string]",
        "total_amount": "number",
        "total_discount": "number",
        "total_tax": "number",
        "items": [
            {
            "name": "string",
            "quantity": {
                "amount": "number",
                "unit_of_measurement": "enumeration[pcs, kg, lb, g]"
            },
            "price": "number",
            "discount": "optional[number]"
            }
        ]
        }

        #Additional Notes:

        1. Handle receipts in various languages and from different countries.
        2. Pay special attention to formatting differences and edge cases, such as multi-line item names, missing currency symbols, and variations in quantity placement.
        3. **Handling Units:**
        - For items sold by weight, extract the numerical weight and normalize the unit to one of the supported values (`kg`, `g`, or `lb`).
        - For items sold by piece, ensure that the unit is set to `"pcs"` and that the number accurately reflects the quantity purchased.
        - When faced with ambiguous formatting or missing quantity information for items that appear to be sold by pieces, default the quantity to 1.
        4. Always ensure the output is well-structured and follows the JSON format provided.
        5. Return the full JSON object with all available information. If any information is unclear or missing, include it as `"unknown"` or `"not available"` in the output.
        6. Do not add any additional text outside the JSON output.
        """
        
        # Make API call to Qwen model
        response = client.chat.completions.create(
            model="qwen/qwen2.5-vl-72b-instruct:free",
            messages=[
                {"role": "user", "content": [
                    {"type": "text", "text": prompt_text},
                    {
                        "type": "image_url",
                        "image_url": {
                            "url": f"data:image/jpeg;base64,{encoded_image}"
                        }
                    }
                ]}
            ],
        )
        
        # Extract and parse the response
        content = response.choices[0].message.content.replace("```", "").replace("json", "")
        print(content)
        
        # Try to parse the response as JSON
        try:
            return json.loads(content)
        except json.JSONDecodeError:
            raise HTTPException(
                status_code=500,
                detail="Failed to parse model response as JSON"
            )
            
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process image: {str(e)}"
        ) 
    

def extract_json_from_image_premise(image_path: str) -> Dict[str, Any]:
    """
    Extract JSON data from an image using Qwen model.
    
    Args:
        image_path: Path to the image file
        
    Returns:
        Dictionary containing the extracted JSON data
        
    Raises:
        HTTPException: If the API call fails or returns invalid data
    """
    try:
        # Encode image
        encoded_image = encode_image(image_path)
        print("Encoded image")

        # Prepare the request
        files = {"file": encoded_image}
        params = {"prompt": "Extract data from the image in JSON format. Return well formed JSON and only it."}

        # Send the request
        response = requests.post("http://qwen:80/generate", files=files, params=params)

        # Check if the request was successful
        if response.status_code == 200:
            # Extract the response content
            response_content = response.json()["response"]
            print(response_content)
            return json.loads(response_content)
        else:
            raise HTTPException(
                status_code=response.status_code,
                detail=f"Failed to extract JSON from image: {response.text}"
            )
    except Exception as e:
        raise HTTPException(
            status_code=500,
            detail=f"Failed to process image: {str(e)}"
        )

================
File: app/src/process/schemas.py
================
from typing import List, Optional
from uuid import UUID
from pydantic import BaseModel
from datetime import datetime

class ImageUploadResponse(BaseModel):
    image_id: str
    status: str

class ImageStatus(BaseModel):
    image_id: str
    s3_key: str
    status: str
    result_json: str
    created_at: datetime

class PaginatedImageResponse(BaseModel):
    images: List[ImageStatus]
    next_cursor: Optional[str] = None

class ImageListParams(BaseModel):
    cursor: Optional[str] = None
    limit: int = 10

================
File: app/src/routers/auth_router.py
================
from fastapi import APIRouter, HTTPException, status
from ..models.connector import connector
from ..models import user
from ..auth import security
from ..auth.schemas import UserRegisterIn, UserLoginIn, TokenOut, RefreshTokenIn

auth_router = APIRouter(tags=["auth"])

@auth_router.post("/register", response_model=TokenOut)
async def register(user_input: UserRegisterIn):
    with connector.engine.begin() as conn:
        if user.get_user_by_email(conn, user_input.email):
            raise HTTPException(status_code=400, detail="Email already registered")
        
        new_user = user.create_user(conn, user_input.email, user_input.password)
        access_token = security.create_access_token({"sub": str(new_user.id)})
        refresh_token = security.create_refresh_token({"sub": str(new_user.id)})

        return {"access_token": access_token, "refresh_token": refresh_token}

@auth_router.post("/login", response_model=TokenOut)
async def login(user_input: UserLoginIn):
    with connector.engine.begin() as conn:
        db_user = user.get_user_by_email(conn, user_input.email)
        if not db_user or not user.verify_password(user_input.password, db_user.hashed_password):
            raise HTTPException(status_code=401, detail="Invalid credentials")

        access_token = security.create_access_token({"sub": str(db_user.id)})
        refresh_token = security.create_refresh_token({"sub": str(db_user.id)})

        return {"access_token": access_token, "refresh_token": refresh_token}

@auth_router.post("/refresh", response_model=TokenOut)
async def refresh_token(data: RefreshTokenIn):
    token = data.refresh_token
    payload = security.verify_token(token)

    if not payload or "sub" not in payload:
        raise HTTPException(status_code=401, detail="Invalid or expired refresh token")

    new_access_token = security.create_access_token({"sub": str(payload["sub"])})
    new_refresh_token = security.create_refresh_token({"sub": str(payload["sub"])})

    return {
        "access_token": new_access_token,
        "refresh_token": new_refresh_token,
        "token_type": "bearer"
    }

================
File: app/src/routers/image_router.py
================
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
import io
import os

from ..bucket_init import s3, bucket_name
from ..auth.security import get_current_user
from ..models.image import Image

image_router = APIRouter(tags=["Image"])

@image_router.get("/get-image")
async def get_image(image_id: str):
    try:
        image_model = Image()
        image = image_model.get_by_id(image_id)
        # Get the image from S3
        response = s3.get_object(Bucket=bucket_name, Key=image.s3_key)
        
        # Extract original filename from s3_key
        original_filename = image.s3_key.split('/')[-1]
        
        # Create a streaming response with filename in headers
        return StreamingResponse(
            io.BytesIO(response['Body'].read()),
            media_type=response['ContentType'],
            headers={
                'Content-Disposition': f'attachment; filename="{original_filename}"'
            }
        )
    except Exception as e:
        raise HTTPException(status_code=404, detail=f"Image not found: {str(e)}")

================
File: app/src/routers/process_router.py
================
import os
import uuid
from fastapi import APIRouter, UploadFile, File, Depends, HTTPException, BackgroundTasks, Query
from typing import List
import boto3
import tempfile

from ..auth.security import get_current_user
from ..process.schemas import ImageUploadResponse, ImageStatus, PaginatedImageResponse, ImageListParams
from ..models.image import Image
from ..process.image_processor import extract_json_from_image_cloud, extract_json_from_image_premise

process_router = APIRouter(tags=["process"])

# S3 client config
s3 = boto3.client(
    "s3",
    endpoint_url=os.environ["S3_ENDPOINT"],
    aws_access_key_id=os.environ["S3_ACCESS_KEY"],
    aws_secret_access_key=os.environ["S3_SECRET_KEY"],
)
S3_BUCKET = os.environ["S3_BUCKET"]

def background_processing(image_id: str, s3_key: str, workload: str):
    image_model = Image()
    try:
        # Update status to in_process
        print("Updated to in_process")
        image_model.update_status(image_id, "in_process")
        
        # Download image from S3 to temporary file
        with tempfile.NamedTemporaryFile(suffix=".jpg", delete=False) as temp_file:
            s3.download_file(S3_BUCKET, s3_key, temp_file.name)
            
            # Process image and extract JSON
            if workload == "cloud":
                extracted_data = extract_json_from_image_cloud(temp_file.name)
            else:
                extracted_data = extract_json_from_image_premise(temp_file.name)
            # Update status to finished
            image_model.update_status(image_id, "finished", extracted_data)
            
    except Exception as e:
        # Update status to error if something goes wrong
        image_model.update_status(image_id, "error")
        image_model.update_error(image_id, str(e))
        raise

@process_router.post("/upload-images", response_model=List[ImageUploadResponse])
async def upload_images(
    background_tasks: BackgroundTasks,
    files: List[UploadFile] = File(...),
    workload: str = "cloud",
    current_user: str = Depends(get_current_user)
):
    if len(files) > 5:
        raise HTTPException(status_code=400, detail="Maximum 5 files allowed.")

    image_model = Image()
    results = []

    for file in files:
        file_id = str(uuid.uuid4())
        s3_key = f"{current_user}/{file_id}/{file.filename}"

        # Upload to S3
        s3.upload_fileobj(file.file, S3_BUCKET, s3_key)

        # Create image record
        result = image_model.create(current_user, s3_key, workload)
        results.append(ImageUploadResponse(image_id=result["id"], status=result["status"]))

        # Background task to process image
        background_tasks.add_task(background_processing, str(result["id"]), s3_key, workload)

    return results

@process_router.get("/images/list", response_model=PaginatedImageResponse)
async def get_image_list(
    current_user: str = Depends(get_current_user),
    cursor: str = None,
    limit: int = Query(10, ge=1, le=100)
):
    image_model = Image()
    images, next_cursor = image_model.get_by_user(current_user, cursor, limit)
    
    return PaginatedImageResponse(
        images=images,
        next_cursor=next_cursor
    )

================
File: app/src/routers/token_router.py
================
from typing import Optional
from fastapi import APIRouter, Depends, HTTPException
from sqlalchemy.orm import Session
from ..models.token import create_token
from ..models.connector import connector
from ..token.schemas import TokenResponse, TokenCreateRequest
from ..auth.security import get_current_user

token_router = APIRouter(tags=["token"])

@token_router.post("/token", response_model=TokenResponse)
async def create_token_endpoint(
    request: TokenCreateRequest,
    user_id: str = Depends(get_current_user)
):
    try:
        result = create_token(connector, user_id, request.days_valid)
        return TokenResponse(
            token=result["token"],
            expires_at=result["expires_at"]
        )
    except Exception as e:
        raise HTTPException(status_code=400, detail=str(e))

================
File: app/src/token/schemas.py
================
from datetime import datetime
from typing import Optional
from pydantic import BaseModel, Field

class TokenCreateRequest(BaseModel):
    days_valid: Optional[int] = Field(None, description="Number of days the token should be valid for. If not specified, token will be infinite")

class TokenResponse(BaseModel):
    token: str = Field(..., description="Generated API token")
    expires_at: Optional[datetime] = Field(None, description="Token expiration date. None if token is infinite")

================
File: app/src/bucket_init.py
================
import os
import boto3
from botocore.exceptions import ClientError

bucket_name = os.environ["S3_BUCKET"]

s3 = boto3.client(
    "s3",
    endpoint_url=os.environ["S3_ENDPOINT"],
    aws_access_key_id=os.environ["S3_ACCESS_KEY"],
    aws_secret_access_key=os.environ["S3_SECRET_KEY"],
)

try:
    s3.create_bucket(Bucket=bucket_name)
    print(f"✅ Bucket '{bucket_name}' created or already exists.")
except ClientError as e:
    if e.response['Error']['Code'] == 'BucketAlreadyOwnedByYou':
        print(f"Bucket '{bucket_name}' already exists.")
    else:
        print(f"Error creating bucket: {e}")

================
File: app/src/constants.py
================
BASE_POSTGRES_TRANSACTIONS_DIRECTORY = 'src/models/postgresql'

================
File: app/src/main.py
================
import logging

from aiomisc.log import basic_config

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware

from .routers.auth_router import auth_router
from .routers.process_router import process_router
from .routers.image_router import image_router
from .routers.token_router import token_router

try:
    from . import bucket_init
except Exception as e:
    print(f"❌ Failed to initialize bucket: {e}")

tags_metadata = [
    {
        "name": "FastApi template",
        "description": "Main API schema.",
    },
]

app = FastAPI(openapi_tags=tags_metadata)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(auth_router)
app.include_router(process_router)
app.include_router(image_router)
app.include_router(token_router)

basic_config(logging.DEBUG, buffered=True)

================
File: app/Dockerfile
================
FROM python:3.10

WORKDIR /code

COPY ./requirements.txt /code/requirements.txt

RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt

COPY ./src /code/src

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "80", "--proxy-headers"]

================
File: app/requirements.txt
================
aiofiles==24.1.0
aiomisc==17.3.41
annotated-types==0.6.0
anyio==4.2.0
boto3==1.38.7
botocore==1.38.7
certifi==2025.4.26
charset-normalizer==3.4.2
click==8.1.7
colorlog==6.8.0
distro==1.9.0
dnspython==2.7.0
ecdsa==0.19.1
email_validator==2.2.0
exceptiongroup==1.2.0
fastapi==0.108.0
greenlet==3.0.3
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.6
jiter==0.9.0
jmespath==1.0.1
openai==1.77.0
psycopg2-binary==2.9.10
pyasn1==0.4.8
pydantic==2.5.3
pydantic_core==2.14.6
python-dateutil==2.9.0.post0
python-dotenv==1.0.0
python-jose==3.4.0
python-multipart==0.0.20
requests==2.32.3
rsa==4.9.1
s3transfer==0.12.0
six==1.17.0
sniffio==1.3.0
SQLAlchemy==2.0.25
starlette==0.32.0.post1
tqdm==4.67.1
typing_extensions==4.13.2
urllib3==2.4.0
uvicorn==0.25.0

================
File: database/migrations/V01__init_schema.sql
================
CREATE SCHEMA app;

================
File: database/migrations/V02__table_users copy.sql
================
CREATE TABLE IF NOT EXISTS app.users (
    id TEXT PRIMARY KEY,
    email VARCHAR(255) UNIQUE NOT NULL,
    hashed_password TEXT NOT NULL,
    is_active BOOLEAN DEFAULT TRUE
);

================
File: database/migrations/V03__table_images.sql
================
CREATE TABLE app.images (
    id TEXT PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id TEXT NOT NULL,
    s3_key TEXT NOT NULL,
    status TEXT NOT NULL CHECK (status IN ('created', 'in_process', 'finished', 'error')),
    status_reason TEXT,
    result_json JSONB DEFAULT '{}'::jsonb,
    created_at TIMESTAMP DEFAULT NOW()
);

================
File: database/migrations/V04__table_tokens.sql
================
CREATE TABLE IF NOT EXISTS app.tokens (
    id TEXT PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id TEXT NOT NULL,
    token TEXT NOT NULL UNIQUE,
    created_at TIMESTAMP NOT NULL DEFAULT NOW(),
    expires_at TIMESTAMP
);

================
File: database/migrations/V05__images_workload.sql
================
ALTER TABLE app.images
ADD workload TEXT NOT NULL CHECK (workload IN ('on_premise', 'cloud')) DEFAULT 'cloud';

================
File: database/Dockerfile
================
FROM postgres:14.10

RUN apt-get update

RUN apt-get install -y python3 python3-pip python3-venv

RUN python3 -m venv .venv

RUN . .venv/bin/activate

# psycopg2 fix aparently
RUN apt-get install -y libpq-dev

RUN .venv/bin/pip3 install --no-cache-dir --upgrade yandex-pgmigrate

COPY ./migrations /migrations

================
File: nginx/nginx.conf
================
events {
    worker_connections 1024;
}

http {
    log_format main '$remote_addr - $remote_user [$time_local] "$request" '
                    '$status $body_bytes_sent "$http_referer" '
                    '"$http_user_agent"';
    
    access_log /var/log/nginx/access.log main;
    error_log /var/log/nginx/error.log;

    include /etc/nginx/mime.types;

    default_type application/octet-stream;

    server {
        listen 80 default_server;
        # server_name localhost;

        location /api {
            proxy_pass http://readonly_backend:80;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }

        location / {
            proxy_pass http://app:80;

            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }
}

================
File: qwen/Dockerfile
================
FROM python:3.10-slim

# System deps
RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*

# Python deps
RUN pip install --no-cache-dir \
    torch torchvision torchaudio \
    transformers accelerate \
    fastapi uvicorn

# Copy app code
COPY . /app
WORKDIR /app

CMD ["uvicorn", "server:app", "--host", "0.0.0.0", "--port", "80"]

================
File: qwen/server.py
================
from fastapi import FastAPI, UploadFile
from transformers import AutoProcessor, AutoModelForVision2Seq
import torch
from PIL import Image
import io

app = FastAPI()

processor = AutoProcessor.from_pretrained("Qwen/Qwen2-VL-2B-Instruct")
model = AutoModelForVision2Seq.from_pretrained("Qwen/Qwen2-VL-2B-Instruct").to("cuda" if torch.cuda.is_available() else "cpu")

@app.post("/generate")
async def generate(file: UploadFile, prompt: str):
    image = Image.open(io.BytesIO(await file.read())).convert("RGB")
    inputs = processor(prompt=prompt, images=image, return_tensors="pt").to(model.device)
    generated_ids = model.generate(**inputs)
    result = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return {"response": result}

================
File: readonly_backend/src/auth/schemas.py
================
from pydantic import BaseModel, EmailStr, Field

# ---------- INPUT MODELS ----------

class UserRegisterIn(BaseModel):
    email: EmailStr
    password: str = Field(min_length=6)

class UserLoginIn(BaseModel):
    email: EmailStr
    password: str = Field(min_length=6)

class RefreshTokenIn(BaseModel):
    refresh_token: str

# ---------- OUTPUT MODELS ----------

class TokenOut(BaseModel):
    access_token: str
    refresh_token: str
    token_type: str = "bearer"

class UserOut(BaseModel):
    id: int
    email: EmailStr

================
File: readonly_backend/src/auth/security.py
================
import os
from datetime import datetime, timedelta, timezone
from jose import jwt, JWTError

from fastapi import Request, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
from sqlalchemy import text

from ..models.connector import connector

from ..constants import BASE_POSTGRES_TRANSACTIONS_DIRECTORY

security_scheme = HTTPBearer(auto_error=False)

async def get_current_user(request: Request):
    credentials: HTTPAuthorizationCredentials = await security_scheme(request)
    if credentials is None or credentials.scheme.lower() != "bearer":
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Not authenticated")

    token = credentials.credentials
    user_id = get_user_id(connector, token)
    if user_id is None:
        raise HTTPException(status_code=status.HTTP_401_UNAUTHORIZED, detail="Invalid token")

    return user_id  # return user id

def get_user_id(connector, token: str):
    with connector.engine.begin() as conn:     
        with open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/get_user_id.sql") as f:
            query = text(f.read())
            result = conn.execute(
                query,
                {
                    "token": token,
                }
            ).fetchone()

            return result.user_id if result else None

================
File: readonly_backend/src/models/postgresql/get_user_id.sql
================
SELECT user_id FROM app.tokens WHERE token = :token AND (expires_at IS NULL OR expires_at >= NOW()) LIMIT 1;

================
File: readonly_backend/src/models/postgresql/images_get_by_id.sql
================
SELECT *
FROM app.images
WHERE id = :image_id
LIMIT 1;

================
File: readonly_backend/src/models/postgresql/images_get_by_user.sql
================
SELECT id, s3_key, status, result_json, created_at
FROM app.images
WHERE user_id = :user_id AND (:cursor_timestamp IS NULL OR created_at < :cursor_timestamp)
ORDER BY created_at DESC
LIMIT :limit;

================
File: readonly_backend/src/models/connector.py
================
import os

from sqlalchemy import create_engine

DB_CONTAINER_NAME='database'

class DBConnector:
    def __init__(self):
        user = os.environ.get('PGUSER')
        password = os.environ.get('PGPASSWORD')
        host = DB_CONTAINER_NAME
        port = os.environ.get('PGPORT')
        db = os.environ.get('PGDATABASE')

        database_url = f'postgresql://{user}:{password}@{host}:{port}/{db}'

        self.engine = create_engine(
            database_url
        )

connector = DBConnector()

================
File: readonly_backend/src/models/image.py
================
from typing import Tuple, List
from datetime import datetime

from sqlalchemy import text

from ..models.schemas import ImageStatus
from ..models.connector import connector
from ..constants import BASE_POSTGRES_TRANSACTIONS_DIRECTORY

def get_by_user(user_id: str, cursor: str = None, limit: int = 10) -> Tuple[List[ImageStatus], str]:
        """
        Get paginated images for a specific user.
        Returns: (images, next_cursor)
        """
        with connector.engine.begin() as conn:
            # If cursor is provided, parse it as timestamp
            cursor_timestamp = None
            if cursor:
                try:
                    cursor_timestamp = datetime.fromisoformat(cursor)
                except ValueError:
                    cursor_timestamp = None

            select_sql = open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/images_get_by_user.sql").read()
            query = text(select_sql)
            
            params = {"user_id": user_id, "limit": limit, "cursor_timestamp": cursor_timestamp}
            
            results = conn.execute(query, params).fetchall()
            
            # Get the next cursor (timestamp of the last record)
            next_cursor = None
            if results:
                next_cursor = results[-1].created_at.isoformat()

            images = [
                ImageStatus(
                    image_id=str(row.id),
                    s3_key=str(row.s3_key),
                    status=str(row.status),
                    result_json=str(row.result_json),
                    created_at=row.created_at
                )
                for row in results
            ]
            
            return images, next_cursor 
        
def get_by_id(image_id: str) -> ImageStatus:
        """
        Get image by it's id.
        Returns: ImageStatus
        """
        with connector.engine.begin() as conn:
            select_sql = open(f"{BASE_POSTGRES_TRANSACTIONS_DIRECTORY}/images_get_by_id.sql").read()
            query = text(select_sql)
            
            params = {"image_id": image_id}
            
            result = conn.execute(query, params).fetchone()
            
            if not result:
                return None
                
            return ImageStatus(
                image_id=str(result.id),
                s3_key=str(result.s3_key), 
                status=str(result.status),
                result_json=str(result.result_json),
                created_at=result.created_at
            )

================
File: readonly_backend/src/models/schemas.py
================
from pydantic import BaseModel, EmailStr, Field
from typing import List, Optional
from datetime import datetime

class ImageStatus(BaseModel):
    image_id: str
    s3_key: str
    status: str
    result_json: str
    created_at: datetime

class PaginatedImageResponse(BaseModel):
    images: List[ImageStatus]
    next_cursor: Optional[str] = None

class ImageListParams(BaseModel):
    cursor: Optional[str] = None
    limit: int = 10

================
File: readonly_backend/src/routers/read_router.py
================
from typing import Optional

from fastapi import APIRouter, HTTPException, Query, status, Depends
from ..models.connector import DBConnector
from ..auth.security import get_current_user
from ..models.schemas import PaginatedImageResponse, ImageListParams
from ..models.image import get_by_user, get_by_id

read_router = APIRouter(tags=["read"], prefix="/api")

@read_router.post("/list", response_model=PaginatedImageResponse)
async def list_images(
    params: ImageListParams,
    user_id: str =  Depends(get_current_user)):
    images, next_cursor = get_by_user(user_id, params.cursor, params.limit)
    
    return PaginatedImageResponse(
        images=images,
        next_cursor=next_cursor
    )

@read_router.get("/image")
async def get_image_data(image_id: str, _: str = Depends(get_current_user)):
    image = get_by_id(image_id)
    return image

================
File: readonly_backend/src/constants.py
================
BASE_POSTGRES_TRANSACTIONS_DIRECTORY = 'src/models/postgresql'

================
File: readonly_backend/src/main.py
================
from fastapi import FastAPI, APIRouter
from fastapi.middleware.cors import CORSMiddleware

from .routers.read_router import read_router

app = FastAPI(title="Readonly Backend")
router = APIRouter(prefix="/api")

@router.get("/")
async def root():
    return {"message": "Readonly Backend API"}

app = FastAPI(title="Readonly Backend")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(read_router)

================
File: readonly_backend/Dockerfile
================
FROM python:3.10

WORKDIR /code

COPY ./requirements.txt /code/requirements.txt

RUN pip install --no-cache-dir --upgrade -r /code/requirements.txt

COPY ./src /code/src

CMD ["uvicorn", "src.main:app", "--host", "0.0.0.0", "--port", "80", "--proxy-headers"]

================
File: readonly_backend/requirements.txt
================
aiofiles==24.1.0
aiomisc==17.3.41
annotated-types==0.6.0
anyio==4.2.0
boto3==1.38.7
botocore==1.38.7
certifi==2025.4.26
click==8.1.7
colorlog==6.8.0
distro==1.9.0
dnspython==2.7.0
ecdsa==0.19.1
email_validator==2.2.0
exceptiongroup==1.2.0
fastapi==0.108.0
greenlet==3.0.3
h11==0.16.0
httpcore==1.0.9
httpx==0.28.1
idna==3.6
jiter==0.9.0
jmespath==1.0.1
openai==1.77.0
psycopg2-binary==2.9.10
pyasn1==0.4.8
pydantic==2.5.3
pydantic_core==2.14.6
python-dateutil==2.9.0.post0
python-dotenv==1.0.0
python-jose==3.4.0
python-multipart==0.0.20
rsa==4.9.1
s3transfer==0.12.0
six==1.17.0
sniffio==1.3.0
SQLAlchemy==2.0.25
starlette==0.32.0.post1
tqdm==4.67.1
typing_extensions==4.13.2
urllib3==2.4.0
uvicorn==0.25.0

================
File: .gitignore
================
# Created by https://www.toptal.com/developers/gitignore/api/venv,python
# Edit at https://www.toptal.com/developers/gitignore?templates=venv,python

### Python ###
# Byte-compiled / optimized / DLL files
__pycache__/
*.py[cod]
*$py.class

# C extensions
*.so

# Distribution / packaging
.Python
build/
develop-eggs/
dist/
downloads/
eggs/
.eggs/
lib/
lib64/
parts/
sdist/
var/
wheels/
share/python-wheels/
*.egg-info/
.installed.cfg
*.egg
MANIFEST

# PyInstaller
#  Usually these files are written by a python script from a template
#  before PyInstaller builds the exe, so as to inject date/other infos into it.
*.manifest
*.spec

# Installer logs
pip-log.txt
pip-delete-this-directory.txt

# Unit test / coverage reports
htmlcov/
.tox/
.nox/
.coverage
.coverage.*
.cache
nosetests.xml
coverage.xml
*.cover
*.py,cover
.hypothesis/
.pytest_cache/
cover/

# Translations
*.mo
*.pot

# Django stuff:
*.log
local_settings.py
db.sqlite3
db.sqlite3-journal

# Flask stuff:
instance/
.webassets-cache

# Scrapy stuff:
.scrapy

# Sphinx documentation
docs/_build/

# PyBuilder
.pybuilder/
target/

# Jupyter Notebook
.ipynb_checkpoints

# IPython
profile_default/
ipython_config.py

# pyenv
#   For a library or package, you might want to ignore these files since the code is
#   intended to run in multiple environments; otherwise, check them in:
# .python-version

# pipenv
#   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
#   However, in case of collaboration, if having platform-specific dependencies or dependencies
#   having no cross-platform support, pipenv may install dependencies that don't work, or not
#   install all needed dependencies.
#Pipfile.lock

# poetry
#   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
#   This is especially recommended for binary packages to ensure reproducibility, and is more
#   commonly ignored for libraries.
#   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
#poetry.lock

# pdm
#   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
#pdm.lock
#   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
#   in version control.
#   https://pdm.fming.dev/#use-with-ide
.pdm.toml

# PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
__pypackages__/

# Celery stuff
celerybeat-schedule
celerybeat.pid

# SageMath parsed files
*.sage.py

# Environments
.env
.venv
env/
venv/
ENV/
env.bak/
venv.bak/

# Spyder project settings
.spyderproject
.spyproject

# Rope project settings
.ropeproject

# mkdocs documentation
/site

# mypy
.mypy_cache/
.dmypy.json
dmypy.json

# Pyre type checker
.pyre/

# pytype static type analyzer
.pytype/

# Cython debug symbols
cython_debug/

# PyCharm
#  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
#  be found at https://github.com/github/gitignore/blob/main/Global/JetBrains.gitignore
#  and can be added to the global gitignore or merged into this file.  For a more nuclear
#  option (not recommended) you can uncomment the following to ignore the entire idea folder.
#.idea/

### Python Patch ###
# Poetry local configuration file - https://python-poetry.org/docs/configuration/#local-configuration
poetry.toml

# ruff
.ruff_cache/

# LSP config files
pyrightconfig.json

### venv ###
# Virtualenv
# http://iamzed.com/2009/05/07/a-primer-on-virtualenv/
[Bb]in
[Ii]nclude
[Ll]ib
[Ll]ib64
[Ll]ocal
[Ss]cripts
pyvenv.cfg
pip-selfcheck.json

# End of https://www.toptal.com/developers/gitignore/api/venv,python

================
File: docker-compose.yml
================
services:
  app:
    container_name: app
    build: ./app
    env_file:
      - .env
    depends_on:
      - database
      - minio
    networks:
      - app-network
    restart: always

  readonly_backend:
    container_name: readonly_backend
    build: ./readonly_backend
    env_file:
      - .env
    depends_on:
      - database
      - minio
    networks:
      - app-network
    restart: always

  # qwen:
  #   build:
  #     context: ./qwen
  #   container_name: qwen
  #   expose:
  #     - "80"   # Expose only to internal docker network
  #   networks:
  #     - app-network

  database:
    container_name: database
    build: ./database
    volumes:
      - database:/var/lib/postgresql/data/
    env_file:
      - .env
    ports:
      - 5432:5432
    healthcheck:
      test: [ "CMD-SHELL", "pg_isready -U postgres" ]
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - app-network
    restart: always

  nginx:
    container_name: nginx
    image: nginx:latest
    restart: always
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf
    ports:
      - 80:80
    networks:
      - app-network
    depends_on:
      - app
      - readonly_backend

  minio:
    container_name: minio
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    env_file:
      - .env
    volumes:
      - minio-data:/data
    networks:
      - app-network
    restart: always

volumes:
  database:
  minio-data:

networks:
  app-network:

================
File: LICENSE
================
MIT License

Copyright (c) 2024 Sergey Lokhmatikov

Permission is hereby granted, free of charge, to any person obtaining a copy
of this software and associated documentation files (the "Software"), to deal
in the Software without restriction, including without limitation the rights
to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
copies of the Software, and to permit persons to whom the Software is
furnished to do so, subject to the following conditions:

The above copyright notice and this permission notice shall be included in all
copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
SOFTWARE.

================
File: startup.sh
================
#!/bin/bash

docker compose up --build --detach

docker exec -it database /bin/bash -c ".venv/bin/python3 -m pgmigrate -c '' migrate -t latest"
